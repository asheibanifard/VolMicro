================================================================================
EXPERIMENT LOG - v014
================================================================================
Date: January 30, 2026
Output Directory: output/sparse_gate_gaussian/v014

================================================================================
DATA
================================================================================
Volume: 10-2900-control-cell-05_cropped_corrected.tif
Shape: (100, 647, 813) = 52,601,100 voxels
Gate Checkpoint: tops_gate_step_020000.pt
Gate Tau: 0.5
Gate Occupancy: 16.07% (8,452,906 / 52,601,100 voxels)

================================================================================
MODEL CONFIGURATION
================================================================================
Initial Gaussians: 20,000
Final Gaussians: 19,709
Max Gaussians: 50,000
Intensity Activation: sigmoid
Parameters per Gaussian: 10 (position:3, scale:3, rotation:4, intensity:1)

================================================================================
TRAINING CONFIGURATION
================================================================================
Epochs: 2,000
Learning Rate: 0.01
Finetune LR: 0.001
Sampling: Enabled (5% of gated voxels = 422,645 samples/epoch)
KNN K: 80

Loss Weights:
  - lambda_sparsity: 0.001
  - lambda_overlap: 0.0
  - lambda_smoothness: 0.0
  - Edge boost: 3.0

================================================================================
DENSIFICATION THRESHOLDS
================================================================================
densify_grad_threshold: 0.0005 (higher = less aggressive)
densify_scale_threshold: 0.005 (clone small < 0.005, split large > 0.005)
densify_interval: 200 epochs
densify_start: 200
densify_stop: 5000

================================================================================
PRUNING THRESHOLDS
================================================================================
prune_intensity_threshold: 0.02 (prune if sigmoid(intensity) < 2%)
prune_scale_threshold: 0.05 (prune if max_scale > 5% of volume)

================================================================================
RESULTS
================================================================================
Final PSNR (sparse, gated voxels): 34.68 dB
Final Loss: 0.000556
Training Time: ~17.5 minutes (1.9 it/s average)

Densification Summary (no clone/split due to high grad_threshold):
  @ 200:  clone=0, split=0, prune=0,   N: 20000 -> 20000
  @ 400:  clone=0, split=0, prune=4,   N: 20000 -> 19996
  @ 600:  clone=0, split=0, prune=13,  N: 19996 -> 19983
  @ 800:  clone=0, split=0, prune=9,   N: 19983 -> 19974
  @ 1000: clone=0, split=0, prune=12,  N: 19974 -> 19962
  @ 1200: clone=0, split=0, prune=17,  N: 19962 -> 19945
  @ 1400: clone=0, split=0, prune=16,  N: 19945 -> 19929
  @ 1600: clone=0, split=0, prune=11,  N: 19929 -> 19918
  @ 1800: clone=0, split=0, prune=209, N: 19918 -> 19709

Gradient Statistics:
  Max gradient observed: ~8e-05 (below threshold 5e-04)
  Mean gradient: ~2e-05 to 6e-06 (decreasing over training)

================================================================================
NOTES
================================================================================
- grad_threshold=0.0005 was too high - no densification occurred
- Gradients never exceeded threshold (max ~8e-05 << 5e-04)
- Model relied solely on initial 20K Gaussians + pruning
- Despite no densification, achieved good PSNR (34.68 dB)
- Pruning removed ~300 low-contribution Gaussians

RECOMMENDATIONS:
- For densification: use grad_threshold <= 0.0001 (e.g., 0.00005)
- Current settings good for stable optimization without growth
- Could start with more initial Gaussians (50K-100K) if no densification

================================================================================
CHECKPOINTS
================================================================================
checkpoint_epoch_000500.pt
checkpoint_epoch_001000.pt
checkpoint_epoch_001500.pt
checkpoint_epoch_002000.pt
checkpoint_final.pt
