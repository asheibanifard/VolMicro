"""
Loss Functions for Gaussian-Based Volume Data Representation
=============================================================

Implementation based on [21 Dec. 24] Algorithm from research proposal.

Mathematical Framework
----------------------
We represent a 3D volume V(x,y,z) as a mixture of N anisotropic Gaussians:

    V(x,y,z) = Î£áµ¢â‚Œâ‚á´º wáµ¢ Â· G(x,y,z; Î¼áµ¢, Î£áµ¢)

where each Gaussian is defined as:

    G(x,y,z; Î¼, Î£) = exp(-Â½ (p - Î¼)áµ€ Î£â»Â¹ (p - Î¼))

with:
    - p = [x, y, z]áµ€ : query point
    - Î¼áµ¢ âˆˆ â„Â³ : center position of Gaussian i
    - Î£áµ¢ âˆˆ â„Â³Ë£Â³ : covariance matrix (positive semi-definite)
    - wáµ¢ âˆˆ â„ : intensity/weight of Gaussian i

The covariance matrix is parameterized as:
    
    Î£ = R Â· S Â· Sáµ€ Â· Ráµ€

where:
    - R âˆˆ SO(3) : rotation matrix (from quaternion q âˆˆ â„â´, ||q|| = 1)
    - S = diag(sâ‚, sâ‚‚, sâ‚ƒ) : scale matrix with sâ±¼ > 0

Loss Functions
--------------
1. MSE Loss (L_mse) - Main reconstruction loss
2. Sparsity Regularization (L_sparse) - L1 penalty on weights
3. Overlap Regularization (L_overlap) - Penalize Gaussian overlap
4. Smoothness Regularization (L_smooth) - Encourage smooth parameter fields
5. Total Loss: L_total = L_mse + Î»_sÂ·L_sparse + Î»_oÂ·L_overlap + Î»_smÂ·L_smooth

References
----------
[1] Kerbl et al., "3D Gaussian Splatting for Real-Time Radiance Field Rendering", 
    SIGGRAPH 2023
[2] Zwicker et al., "EWA Splatting", IEEE TVCG 2002
[3] Yu et al., "Mip-Splatting: Alias-free 3D Gaussian Splatting", CVPR 2024
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# Try to import FAISS for GPU-accelerated KNN in overlap loss
try:
    import faiss
    import faiss.contrib.torch_utils
    FAISS_AVAILABLE = True
    FAISS_GPU_AVAILABLE = faiss.get_num_gpus() > 0
except ImportError:
    FAISS_AVAILABLE = False
    FAISS_GPU_AVAILABLE = False


class ReconstructionLoss(nn.Module):
    """
    Mean Squared Error Loss for volume reconstruction.
    
    Mathematical Formulation
    ------------------------
    Given M sampled voxel positions and their ground truth values, the MSE loss is:
    
        L_mse = (1/M) Â· Î£â‚–â‚Œâ‚á´¹ (f(xâ‚–, yâ‚–, zâ‚–) - vâ‚–)Â²
    
    where:
        - M : number of sampled voxels (can be full volume or random subset)
        - (xâ‚–, yâ‚–, zâ‚–) : 3D coordinates of voxel k
        - f(Â·) : predicted intensity from Gaussian mixture
        - vâ‚– : ground truth voxel intensity
    
    The predicted value at each point is computed as:
    
        f(x,y,z) = Î£áµ¢â‚Œâ‚á´º wáµ¢ Â· Î±áµ¢ Â· exp(-Â½ dáµ¢áµ€ Î£áµ¢â»Â¹ dáµ¢)
    
    where dáµ¢ = [x,y,z]áµ€ - Î¼áµ¢ is the displacement from Gaussian center i,
    wáµ¢ is the intensity, and Î±áµ¢ âˆˆ [0,1] is the opacity.
    
    Gradient w.r.t. Parameters
    --------------------------
    For position Î¼áµ¢:
        âˆ‚L/âˆ‚Î¼áµ¢ = (2/M) Â· Î£â‚– (f(pâ‚–) - vâ‚–) Â· wáµ¢Î±áµ¢ Â· Gáµ¢(pâ‚–) Â· Î£áµ¢â»Â¹ Â· dáµ¢â‚–
    
    For scale sâ±¼ (via log-scale for numerical stability):
        âˆ‚L/âˆ‚log(sâ±¼) = sâ±¼ Â· âˆ‚L/âˆ‚sâ±¼
    """
    
    def __init__(self):
        super().__init__()
        self.mse = nn.MSELoss(reduction='mean')
    
    def forward(self, predicted: torch.Tensor, ground_truth: torch.Tensor) -> torch.Tensor:
        """
        Compute MSE loss between predicted and ground truth values.
        
        Args:
            predicted: Predicted voxel values of shape (M,) or (D, H, W)
            ground_truth: Ground truth voxel values of shape (M,) or (D, H, W)
            
        Returns:
            MSE loss value (scalar tensor)
            
        Notes:
            - For full-volume training: M = D Ã— H Ã— W
            - For sampled training: M = num_samples (typically 0.1% - 1% of volume)
            - Loss is normalized by M, making it comparable across different sample sizes
        """
        return self.mse(predicted, ground_truth)


class SparsityRegularization(nn.Module):
    """
    Weight Sparsity Regularization (L1 Penalty).
    
    Mathematical Formulation
    ------------------------
    The L1 sparsity regularization encourages sparse weight distributions:
    
        L_sparse = Î»_w Â· Î£áµ¢â‚Œâ‚á´º |wáµ¢|
    
    where:
        - N : number of Gaussians
        - wáµ¢ : intensity/weight of Gaussian i
        - Î»_w : regularization coefficient (hyperparameter)
    
    Motivation
    ----------
    L1 regularization promotes sparsity by penalizing the absolute magnitude
    of weights. This encourages the model to:
    
    1. Use fewer active Gaussians (many weights â†’ 0)
    2. Represent the volume with minimal complexity
    3. Improve generalization by preventing overfitting
    
    The L1 penalty is non-differentiable at w=0, but subgradient methods
    (used by optimizers like Adam) handle this effectively.
    
    Gradient
    --------
        âˆ‚L_sparse/âˆ‚wáµ¢ = Î»_w Â· sign(wáµ¢)
    
    where sign(x) = +1 if x > 0, -1 if x < 0, and âˆˆ [-1,1] if x = 0.
    
    Hyperparameter Selection
    ------------------------
    - Î»_w âˆˆ [1e-4, 1e-2] typical range
    - Higher values â†’ sparser solution, potentially underfitting
    - Lower values â†’ denser solution, better reconstruction
    """
    
    def __init__(self, lambda_w: float = 0.01):
        """
        Args:
            lambda_w: Regularization coefficient (default: 0.01)
        """
        super().__init__()
        self.lambda_w = lambda_w
    
    def forward(self, weights: torch.Tensor) -> torch.Tensor:
        """
        Compute L1 sparsity regularization.
        
        L_sparsity = Î»_w Â· (1/N) Â· Î£áµ¢â‚Œâ‚á´º |wáµ¢|
        
        Note: We normalize by N to keep the loss scale independent of
        the number of Gaussians, making lambda values more intuitive.
        The formula still uses sum (matching proposal) but is normalized.
        
        Args:
            weights: Gaussian weights/intensities of shape (N,)
            
        Returns:
            Sparsity loss value: Î»_w Â· mean(|wáµ¢|)
        """
        N = weights.shape[0]
        return self.lambda_w * torch.sum(torch.abs(weights)) / N


class OverlapRegularization(nn.Module):
    """
    Overlap Regularization to prevent Gaussian redundancy.
    
    Mathematical Formulation
    ------------------------
    The overlap regularization penalizes excessive overlap between Gaussians:
    
        L_overlap = Î»_o Â· Î£áµ¢â‚Œâ‚á´º Î£â±¼>áµ¢ O(Gáµ¢, Gâ±¼)
    
    where O(Gáµ¢, Gâ±¼) measures the overlap between Gaussians i and j.
    
    Overlap Metrics
    ---------------
    Several metrics can quantify Gaussian overlap:
    
    1. **Bhattacharyya Coefficient** (exact, expensive):
        BC(Gáµ¢, Gâ±¼) = âˆ« âˆš(Gáµ¢(x) Â· Gâ±¼(x)) dx
        
        For two Gaussians with means Î¼áµ¢, Î¼â±¼ and covariances Î£áµ¢, Î£â±¼:
        BC = exp(-Â¼ dáµ€ Î£â»Â¹ d) Â· |Î£|^(1/4) / (|Î£áµ¢|^(1/8) Â· |Î£â±¼|^(1/8))
        
        where Î£ = (Î£áµ¢ + Î£â±¼)/2 and d = Î¼áµ¢ - Î¼â±¼
    
    2. **Simplified Distance-Based** (used here, efficient):
        O(Gáµ¢, Gâ±¼) = exp(-||Î¼áµ¢ - Î¼â±¼||Â² / (2(ráµ¢ + râ±¼)Â²))
        
        where ráµ¢ = âˆš(tr(Î£áµ¢)/3) is the effective radius of Gaussian i.
    
    This approximation:
    - Approaches 1 when Gaussians are coincident
    - Decays exponentially with separation
    - Is cheap to compute (O(NÂ²) distance matrix)
    
    Motivation
    ----------
    Without overlap regularization, Gaussians may:
    - Pile up in high-intensity regions
    - Create redundant representations
    - Waste model capacity
    
    With overlap penalty:
    - Gaussians spread to cover volume efficiently
    - Each Gaussian contributes unique information
    - Better utilization of limited Gaussian budget
    
    Computational Complexity
    ------------------------
    - Naive: O(NÂ²) for all pairs
    - With max_pairs sampling: O(max_pairs) 
    - GPU-accelerated distance matrix is efficient for N < 100k
    """
    
    def __init__(self, lambda_o: float = 0.01):
        """
        Args:
            lambda_o: Regularization coefficient (default: 0.01)
        """
        super().__init__()
        self.lambda_o = lambda_o
    
    def forward(
        self,
        positions: torch.Tensor,
        covariance: torch.Tensor,
        knn_k: int = 16,
        max_gaussians: int = 5000
    ) -> torch.Tensor:
        """
        Compute overlap regularization using KNN-based sampling (memory efficient).
        
        Algorithm (memory-efficient O(N*K) instead of O(NÂ²)):
            1. Sample up to max_gaussians if N is large
            2. For each Gaussian, find K nearest neighbors
            3. Compute overlap only with those K neighbors
            4. Sum overlaps (approximation of full pairwise sum)
        
        Args:
            positions: Gaussian centers Î¼áµ¢ of shape (N, 3)
            covariance: Covariance matrices Î£áµ¢ of shape (N, 3, 3)
            knn_k: Number of nearest neighbors to consider (default: 16)
            max_gaussians: Max Gaussians to sample for very large N (default: 5000)
            
        Returns:
            Overlap loss: Î»_o Â· Î£áµ¢ Î£â±¼âˆˆKNN(i) O(Gáµ¢, Gâ±¼)
        """
        N = positions.shape[0]
        
        if N < 2:
            return torch.tensor(0.0, device=positions.device)
        
        # Sample Gaussians if N is too large
        if N > max_gaussians:
            idx = torch.randperm(N, device=positions.device)[:max_gaussians]
            positions = positions[idx]
            covariance = covariance[idx]
            N = max_gaussians
        
        # Compute effective radius: r = âˆš(tr(Î£)/3)
        scales = torch.sqrt(torch.diagonal(covariance, dim1=1, dim2=2).mean(dim=1))  # (N,)
        
        # For speed: sample a small subset of Gaussians for overlap computation
        # Full overlap on all N is O(NÂ²) which is too slow for training
        sample_size = min(1000, N)  # Only use 1000 Gaussians max
        if N > sample_size:
            idx = torch.randperm(N, device=positions.device)[:sample_size]
            positions = positions[idx]
            scales = scales[idx]
            N = sample_size
        
        # Adjust K to not exceed N-1
        K = min(knn_k, N - 1)
        
        # Single vectorized computation (N is small now, ~1000)
        # Pairwise distances: (N, N) - only 1M elements for N=1000
        dist_sq = torch.cdist(positions, positions, p=2).pow(2)  # (N, N)
        
        # Combined scales for all pairs: (N, N)
        combined_r = scales.unsqueeze(1) + scales.unsqueeze(0)  # (N, N)
        
        # Overlap matrix: (N, N)
        overlap = torch.exp(-dist_sq / (2 * combined_r ** 2 + 1e-6))
        
        # Zero diagonal (self-overlap) - use mask instead of inplace
        diag_mask = torch.eye(N, dtype=torch.bool, device=positions.device)
        overlap = overlap.masked_fill(diag_mask, 0.0)
        
        # Sum upper triangle only (each pair once)
        # Normalize by number of pairs: N*(N-1)/2
        num_pairs = N * (N - 1) / 2
        total_overlap = overlap.triu(diagonal=1).sum() / max(1.0, num_pairs)
        
        return self.lambda_o * total_overlap


class SmoothnessRegularization(nn.Module):
    """
    Smoothness Regularization for spatially coherent Gaussian fields.
    
    Mathematical Formulation
    ------------------------
    The smoothness regularization encourages nearby Gaussians to have 
    similar parameters, creating a spatially coherent representation:
    
        L_smooth = Î»_s Â· Î£áµ¢â‚Œâ‚á´º Î£â±¼âˆˆğ’©(i) [ (wáµ¢ - wâ±¼)Â² + ||log(sáµ¢) - log(sâ±¼)||Â² ]
    
    where:
        - ğ’©(i) : k-nearest neighbors of Gaussian i (by position)
        - wáµ¢ : intensity/weight of Gaussian i  
        - sáµ¢ : scale vector of Gaussian i
        - Î»_s : regularization coefficient
    
    Why Log-Scale?
    --------------
    We regularize log(s) instead of s directly because:
    
    1. **Scale invariance**: Penalizing (sâ‚ - sâ‚‚)Â² treats a change from 
       0.01â†’0.02 differently than 0.1â†’0.2, though both are 2Ã— changes.
       Using log: (log(0.02)-log(0.01))Â² = (log(0.2)-log(0.1))Â² = (log 2)Â²
    
    2. **Numerical stability**: Scales span orders of magnitude; 
       log compression prevents large scales from dominating.
    
    3. **Multiplicative regularization**: log-difference corresponds to
       ratio regularization: ||log(sáµ¢/sâ±¼)||Â² penalizes scale ratios.
    
    k-Nearest Neighbors
    -------------------
    We use spatial neighbors (by Î¼áµ¢ position) rather than all pairs because:
    
    1. Distant Gaussians should be independent
    2. O(NÂ·k) complexity vs O(NÂ²) for all pairs
    3. Aligns with physical intuition: nearby regions should be similar
    
    Gradient
    --------
    For weight smoothness:
        âˆ‚L_smooth/âˆ‚wáµ¢ = 2Î»_s Â· Î£â±¼âˆˆğ’©(i) (wáµ¢ - wâ±¼)
        
    This acts as a graph Laplacian smoothing operator.
    
    Connection to Total Variation
    -----------------------------
    This is related to Total Variation (TV) regularization but uses
    L2 norm instead of L1, making it differentiable everywhere and
    encouraging smooth transitions rather than piecewise constant fields.
    """
    
    def __init__(self, lambda_s: float = 0.01, num_neighbors: int = 5):
        """
        Args:
            lambda_s: Regularization coefficient (default: 0.01)
            num_neighbors: Number of nearest neighbors k (default: 5)
        """
        super().__init__()
        self.lambda_s = lambda_s
        self.num_neighbors = num_neighbors
    
    def forward(
        self,
        positions: torch.Tensor,
        weights: torch.Tensor,
        log_scales: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute smoothness regularization over k-nearest neighbor graph.
        
        Algorithm:
            1. Build kNN graph: find k nearest neighbors for each Gaussian
            2. Compute weight differences: Î”wáµ¢â±¼ = wáµ¢ - wâ±¼ for j âˆˆ ğ’©(i)
            3. Compute scale differences: Î”sáµ¢â±¼ = log(sáµ¢) - log(sâ±¼)
            4. Sum squared differences: L = Î»Â·(mean(Î”wÂ²) + mean(Î”sÂ²))
        
        Args:
            positions: Gaussian centers Î¼áµ¢ of shape (N, 3)
            weights: Gaussian weights wáµ¢ of shape (N,)
            log_scales: Log-scales log(sáµ¢) of shape (N, 3)
            
        Returns:
            Smoothness loss: Î»_s Â· (L_weight + L_scale)
        """
        N = positions.shape[0]
        
        if N < 2:
            return torch.tensor(0.0, device=positions.device)
        
        # Compute pairwise Euclidean distances: D[i,j] = ||Î¼áµ¢ - Î¼â±¼||
        distances = torch.cdist(positions, positions)  # (N, N)
        
        # Get k nearest neighbors for each Gaussian (excluding self)
        k = min(self.num_neighbors, N - 1)
        _, indices = torch.topk(distances, k + 1, largest=False, dim=1)
        neighbor_indices = indices[:, 1:]  # (N, k) - exclude self (index 0)
        
        # Discrete gradient approximation: âˆ‡_u G_i â‰ˆ (G_i - G_j) / ||Î¼_i - Î¼_j||
        # L_smoothness = Î»_s Â· Î£áµ¢ ||âˆ‡_u Gáµ¢||Â²
        
        # Get neighbor distances for normalization
        neighbor_distances = distances.gather(1, neighbor_indices)  # (N, k)
        neighbor_distances = neighbor_distances.clamp(min=1e-6)  # Avoid div by zero
        
        # Weight gradient: âˆ‡w â‰ˆ (wáµ¢ - wâ±¼) / d_ij
        weight_neighbors = weights[neighbor_indices]  # (N, k)
        weight_diff = weights.unsqueeze(1) - weight_neighbors  # (N, k)
        weight_grad_sq = (weight_diff / neighbor_distances) ** 2  # (N, k)
        weight_smoothness = torch.sum(weight_grad_sq)  # Î£áµ¢ Î£â±¼âˆˆğ’©(i)
        
        # Scale gradient: âˆ‡s â‰ˆ (log sáµ¢ - log sâ±¼) / d_ij  
        scale_neighbors = log_scales[neighbor_indices]  # (N, k, 3)
        scale_diff = log_scales.unsqueeze(1) - scale_neighbors  # (N, k, 3)
        scale_grad_sq = (scale_diff / neighbor_distances.unsqueeze(-1)) ** 2  # (N, k, 3)
        scale_smoothness = torch.sum(scale_grad_sq)  # Î£áµ¢ ||âˆ‡sáµ¢||Â²
        
        # Normalize by N*k to keep loss scale independent of N and k
        num_edges = N * k
        return self.lambda_s * (weight_smoothness + scale_smoothness) / num_edges


class TotalLoss(nn.Module):
    """
    Total Loss combining reconstruction and regularization terms.
    
    Mathematical Formulation
    ------------------------
    The total loss is a weighted combination of all loss components:
    
        L_total = L_mse + Î»_sÂ·L_sparse + Î»_oÂ·L_overlap + Î»_smÂ·L_smooth
    
    Expanded form:
    
        L_total = (1/M)Â·Î£â‚–(f(pâ‚–) - vâ‚–)Â²           [Reconstruction]
                + Î»_sÂ·Î£áµ¢|wáµ¢|                       [Sparsity]
                + Î»_oÂ·Î£áµ¢<â±¼ O(Gáµ¢,Gâ±¼)               [Overlap]
                + Î»_smÂ·Î£áµ¢ Î£â±¼âˆˆğ’©(i) ||Î¸áµ¢-Î¸â±¼||Â²      [Smoothness]
    
    Hyperparameter Balancing
    ------------------------
    The regularization weights should be chosen such that:
    
    1. L_mse dominates initially (focus on fitting data)
    2. Regularization prevents overfitting/degeneracy
    3. Typical ranges:
        - Î»_s âˆˆ [1e-5, 1e-2] : sparsity
        - Î»_o âˆˆ [1e-5, 1e-2] : overlap
        - Î»_sm âˆˆ [1e-5, 1e-2] : smoothness
    
    Training Dynamics
    -----------------
    Early training: L_mse >> regularization terms
        â†’ Model focuses on reducing reconstruction error
        
    Late training: L_mse â‰ˆ regularization terms (ideally)
        â†’ Regularization refines solution quality
    
    If regularization dominates early:
        â†’ Increase learning rate or decrease Î» values
        
    If regularization has no effect:
        â†’ Increase Î» values or model may be underconstrained
    
    Loss Landscape Considerations
    -----------------------------
    - L_mse: Convex in weights, non-convex in positions/scales
    - L_sparse: Convex (L1 norm), creates sparse optima
    - L_overlap: Non-convex, can have many local minima
    - L_smooth: Convex (quadratic), acts as Laplacian smoother
    
    The combined loss is non-convex, requiring careful initialization
    and learning rate scheduling for good convergence.
    """
    
    def __init__(
        self,
        lambda_sparsity: float = 0.01,
        lambda_overlap: float = 0.01,
        lambda_smoothness: float = 0.01,
        use_sparsity: bool = True,
        use_overlap: bool = True,
        use_smoothness: bool = True
    ):
        """
        Initialize total loss with configurable regularization terms.
        
        Args:
            lambda_sparsity: Weight for L1 sparsity regularization (default: 0.01)
            lambda_overlap: Weight for overlap regularization (default: 0.01)
            lambda_smoothness: Weight for smoothness regularization (default: 0.01)
            use_sparsity: Enable sparsity regularization (default: True)
            use_overlap: Enable overlap regularization (default: True)
            use_smoothness: Enable smoothness regularization (default: True)
            
        Notes:
            Set Î»=0 or use_X=False to disable specific regularization terms.
            Start with small Î» values and increase if needed.
        """
        super().__init__()
        
        self.reconstruction_loss = ReconstructionLoss()
        
        self.use_sparsity = use_sparsity
        self.use_overlap = use_overlap
        self.use_smoothness = use_smoothness
        
        if use_sparsity:
            self.sparsity_loss = SparsityRegularization(lambda_sparsity)
        if use_overlap:
            self.overlap_loss = OverlapRegularization(lambda_overlap)
        if use_smoothness:
            self.smoothness_loss = SmoothnessRegularization(lambda_smoothness)
    
    def forward(
        self,
        predicted: torch.Tensor,
        ground_truth: torch.Tensor,
        model=None
    ) -> dict:
        """
        Compute total loss with all enabled components.
        
        Args:
            predicted: Predicted voxel values of shape (M,) or (D,H,W)
            ground_truth: Ground truth voxel values, same shape as predicted
            model: GaussianVolumeModel instance (required for regularization)
            
        Returns:
            Dictionary containing:
                - 'mse': Reconstruction loss (always present)
                - 'sparsity': L1 sparsity loss (if enabled)
                - 'overlap': Overlap regularization (if enabled)
                - 'smoothness': Smoothness regularization (if enabled)
                - 'total': Sum of all enabled losses
                
        Example:
            >>> loss_fn = TotalLoss(lambda_sparsity=0.01)
            >>> losses = loss_fn(pred, gt, model)
            >>> losses['total'].backward()
        """
        losses = {}
        
        # Main reconstruction loss: L_mse = (1/M)Â·Î£(pred - gt)Â²
        losses['mse'] = self.reconstruction_loss(predicted, ground_truth)
        losses['total'] = losses['mse']
        
        if model is not None:
            # Sparsity regularization: L_sparse = Î»Â·Î£|wáµ¢|
            if self.use_sparsity:
                losses['sparsity'] = self.sparsity_loss(model.weights)
                losses['total'] = losses['total'] + losses['sparsity']
            
            # Overlap regularization: L_overlap = Î»Â·Î£áµ¢<â±¼ O(Gáµ¢,Gâ±¼)
            if self.use_overlap:
                covariance = model.gaussians.get_covariance_matrices()
                losses['overlap'] = self.overlap_loss(model.positions, covariance)
                losses['total'] = losses['total'] + losses['overlap']
            
            # Smoothness regularization: L_smooth = Î»Â·Î£áµ¢ Î£â±¼âˆˆğ’©(i) ||Î¸áµ¢-Î¸â±¼||Â²
            if self.use_smoothness:
                losses['smoothness'] = self.smoothness_loss(
                    model.positions, model.weights, model.log_scales
                )
                losses['total'] = losses['total'] + losses['smoothness']
        
        return losses


def compute_psnr(predicted: torch.Tensor, ground_truth: torch.Tensor) -> float:
    """
    Compute Peak Signal-to-Noise Ratio (PSNR).
    
    Mathematical Definition
    -----------------------
    PSNR measures the ratio between the maximum possible signal power 
    and the power of corrupting noise (reconstruction error):
    
        PSNR = 10 Â· logâ‚â‚€(MAXÂ² / MSE)
             = 20 Â· logâ‚â‚€(MAX / âˆšMSE)
             = 20 Â· logâ‚â‚€(MAX) - 10 Â· logâ‚â‚€(MSE)
    
    where:
        - MAX : maximum possible pixel/voxel value
        - MSE : Mean Squared Error = (1/M)Â·Î£(pred - gt)Â²
    
    Interpretation
    --------------
    PSNR is expressed in decibels (dB). Higher is better.
    
    Typical ranges for image/volume reconstruction:
        - < 20 dB  : Poor quality, significant artifacts
        - 20-30 dB : Acceptable quality
        - 30-40 dB : Good quality
        - > 40 dB  : Excellent quality (often visually lossless)
    
    Relationship to SSIM
    --------------------
    PSNR measures pixel-wise error but doesn't capture perceptual quality.
    SSIM (Structural Similarity Index) is often used alongside PSNR:
    
        SSIM = [l(x,y)]^Î± Â· [c(x,y)]^Î² Â· [s(x,y)]^Î³
    
    where l, c, s measure luminance, contrast, and structure similarity.
    
    For volumetric data (especially microscopy), PSNR is often preferred
    as structural assumptions of SSIM may not apply.
    
    Args:
        predicted: Predicted values (any shape, will be flattened)
        ground_truth: Ground truth values (same shape as predicted)
        
    Returns:
        PSNR value in decibels (dB). Returns inf if MSE = 0.
        
    Example:
        >>> psnr = compute_psnr(reconstructed_volume, original_volume)
        >>> print(f"PSNR: {psnr:.2f} dB")
    """
    mse = F.mse_loss(predicted, ground_truth).item()
    if mse == 0:
        return float('inf')
    
    # Use max of ground truth as the peak signal value
    max_val = ground_truth.max().item()
    
    # PSNR = 20Â·logâ‚â‚€(MAX/âˆšMSE)
    psnr = 20 * torch.log10(torch.tensor(max_val / (mse ** 0.5)))
    return psnr.item()


def compute_ssim(
    predicted: torch.Tensor, 
    ground_truth: torch.Tensor,
    window_size: int = 11,
    C1: float = 0.01**2,
    C2: float = 0.03**2
) -> float:
    """
    Compute Structural Similarity Index (SSIM) for 3D volumes.
    
    Mathematical Definition
    -----------------------
    SSIM compares local patterns of pixel intensities normalized for
    luminance and contrast:
    
        SSIM(x,y) = (2Î¼â‚“Î¼áµ§ + Câ‚)(2Ïƒâ‚“áµ§ + Câ‚‚) / ((Î¼â‚“Â² + Î¼áµ§Â² + Câ‚)(Ïƒâ‚“Â² + Ïƒáµ§Â² + Câ‚‚))
    
    where:
        - Î¼â‚“, Î¼áµ§ : local means
        - Ïƒâ‚“Â², Ïƒáµ§Â² : local variances  
        - Ïƒâ‚“áµ§ : local covariance
        - Câ‚, Câ‚‚ : stability constants (avoid division by zero)
    
    The overall SSIM is averaged over all local windows.
    
    Components
    ----------
    SSIM can be decomposed into three components:
    
    1. Luminance: l(x,y) = (2Î¼â‚“Î¼áµ§ + Câ‚)/(Î¼â‚“Â² + Î¼áµ§Â² + Câ‚)
    2. Contrast:  c(x,y) = (2Ïƒâ‚“Ïƒáµ§ + Câ‚‚)/(Ïƒâ‚“Â² + Ïƒáµ§Â² + Câ‚‚)  
    3. Structure: s(x,y) = (Ïƒâ‚“áµ§ + Câ‚ƒ)/(Ïƒâ‚“Ïƒáµ§ + Câ‚ƒ)
    
    where Câ‚ƒ = Câ‚‚/2. Full SSIM = l Â· c Â· s.
    
    Args:
        predicted: Predicted volume of shape (D, H, W)
        ground_truth: Ground truth volume of shape (D, H, W)
        window_size: Size of local window (default: 11)
        C1: Luminance stability constant
        C2: Contrast stability constant
        
    Returns:
        SSIM value in range [-1, 1]. Higher is better, 1 = identical.
        
    Note:
        This is a simplified implementation. For production use,
        consider pytorch-msssim or skimage.metrics.structural_similarity.
    """
    # Ensure 5D for 3D convolution: (B, C, D, H, W)
    if predicted.dim() == 3:
        predicted = predicted.unsqueeze(0).unsqueeze(0)
        ground_truth = ground_truth.unsqueeze(0).unsqueeze(0)
    
    # Create Gaussian window
    def gaussian_window(size, sigma=1.5):
        coords = torch.arange(size, dtype=torch.float32) - size // 2
        g = torch.exp(-(coords**2) / (2 * sigma**2))
        g = g / g.sum()
        # 3D window = outer product of 1D windows
        window = g.view(-1, 1, 1) * g.view(1, -1, 1) * g.view(1, 1, -1)
        return window.unsqueeze(0).unsqueeze(0)
    
    window = gaussian_window(window_size).to(predicted.device)
    
    # Local means
    mu_x = F.conv3d(predicted, window, padding=window_size//2)
    mu_y = F.conv3d(ground_truth, window, padding=window_size//2)
    
    mu_x_sq = mu_x ** 2
    mu_y_sq = mu_y ** 2
    mu_xy = mu_x * mu_y
    
    # Local variances and covariance
    sigma_x_sq = F.conv3d(predicted**2, window, padding=window_size//2) - mu_x_sq
    sigma_y_sq = F.conv3d(ground_truth**2, window, padding=window_size//2) - mu_y_sq
    sigma_xy = F.conv3d(predicted * ground_truth, window, padding=window_size//2) - mu_xy
    
    # SSIM formula
    numerator = (2 * mu_xy + C1) * (2 * sigma_xy + C2)
    denominator = (mu_x_sq + mu_y_sq + C1) * (sigma_x_sq + sigma_y_sq + C2)
    
    ssim_map = numerator / (denominator + 1e-8)
    
    return ssim_map.mean().item()
